---
title: "Random Forest Final"
author: 
date:
output: html_document
---

```{r}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# Libraries and Data Import
```{r}
library(PRROC)
library(caret)
library(knitr)
library(tidyverse)
library(data.table)
library(caret)
library(ranger)
library(matrixStats)
library(Metrics)
library(ROCR)
library(pROC)

application_train_clean <- as.data.frame(fread("data/application_train_clean.csv"))
application_test_clean <- as.data.frame(fread("data/application_test_clean.csv"))

application_train_smote <- as.data.frame(fread("data/application_train_smote.csv"))
```

# SMOTE Class Balance
```{r}
application_train_smote %>% pull(DEFAULT) %>% table()
application_train_smote %>% pull(DEFAULT) %>% table() %>% prop.table() %>% round(2)
```

# SMOTE Data Partitioning
```{r}
# split data set
creditinTrainsmote <- createDataPartition(application_train_smote$DEFAULT, p=.7, list=FALSE)

# train set
credit_train_smote <- application_train_smote[creditinTrainsmote,]

# test set
credit_test_smote <- application_train_smote[-creditinTrainsmote,]
```


# SMOTE Training the Random Forest
```{r}
# convert target column to factor
credit_train_smote$DEFAULT <- as.factor(credit_train_smote$DEFAULT)
credit_test_smote$DEFAULT <- as.factor(credit_test_smote$DEFAULT)

# train RF model with 500 trees, 10 depth
rf_modelSMOTE <- ranger(formula = DEFAULT ~ ., data = credit_train_smote, 
                   num.trees = 500, max.depth = 10, oob.error = TRUE, , importance = 'impurity', seed = 1234)

# find and print important variables
importance_SMOTE <- sort(rf_modelSMOTE$variable.importance, decreasing = TRUE)
print(importance_SMOTE)
```

# SMOTE Predictions on Train Data
```{r}
# predict classes on train data
predictions_trainSMOTE <- predict(rf_modelSMOTE, credit_train_smote)$predictions
```

## SMOTE Train Metrics
```{r}
# get confusion matrix
conf_matrix_trainSMOTE <- confusionMatrix(predictions_trainSMOTE, credit_train_smote$DEFAULT)
print(conf_matrix_trainSMOTE)
```

# SMOTE Predictions on Test Data
```{r}
# predict on test data
predictions_testSMOTE <- predict(rf_modelSMOTE, credit_test_smote)$predictions
```

## SMOTE Test Metrics
```{r}
# get confusion matrix
conf_matrix_testSMOTE <- confusionMatrix(predictions_testSMOTE, credit_test_smote$DEFAULT)
print(conf_matrix_testSMOTE)
```

# SMOTE AUC Train Set
```{r}
# train RF for AUC calculation
rf_AUCSMOTE <- ranger(formula = DEFAULT ~ ., data = credit_train_smote, 
                   num.trees = 500, max.depth = 10, oob.error = TRUE, probability = TRUE, importance = 'impurity', seed = 1234)

# train probabilities
probs_trainSMOTE <- predict(rf_AUCSMOTE, data = credit_train_smote)$predictions[, 2]

# train predictions
pred_trainSMOTE <- prediction(probs_trainSMOTE, credit_train_smote$DEFAULT)

# train trp and fpr performance metrics
perf_trainSMOTE <- performance(pred_trainSMOTE, measure = "tpr", x.measure = "fpr")

# train AUC performance
auc_perf_trainSMOTE <- performance(pred_trainSMOTE, measure = "auc")

# train AUC calculation
auc_value_trainSMOTE <- auc_perf_trainSMOTE@y.values[[1]]

# print AUC value
print(paste("AUC =", round(auc_value_trainSMOTE, 4)))
```

# SMOTE AUC Test Set
```{r}
# test test probabilities
probs_testSMOTE <- predict(rf_AUCSMOTE, data = credit_test_smote)$predictions[, 2]

# test set predictions
pred_testSMOTE <- prediction(probs_testSMOTE, credit_test_smote$DEFAULT)

# test set tpr and fpr performance
perf_testSMOTE <- performance(pred_testSMOTE, measure = "tpr", x.measure = "fpr")

# test set AUC performance
auc_perf_testSMOTE <- performance(pred_testSMOTE, measure = "auc")

# test set AUC calculation
auc_value_testSMOTE <- auc_perf_testSMOTE@y.values[[1]]

# print AUC value
print(paste("AUC =", round(auc_value_testSMOTE, 4)))
```

Choosing 500 trees, and a depth of 10, we observed a training set accuracy of 0.94, recall of 1.0, precision of 0.88, and an AUC of 0.97.  In the test set we observed similar results across all metrics, implying that there isnâ€™t overfitting in this model, but there could be improvements across all metrics.

# SMOTE Model 2
```{r}
# train RF with 800 trees and 20 depth
rf_model1smote <- ranger(formula = DEFAULT ~ ., data = credit_train_smote, , importance = 'impurity',
                   num.trees = 800, max.depth = 20, seed = 1234)

# train predictions
predictions_train1smote <- predict(rf_model1smote, credit_train_smote)$predictions

# test predictions
predictions_test1smote <- predict(rf_model1smote, credit_test_smote)$predictions

# conf matrix train
conf_matrix_train1smote <- confusionMatrix(predictions_train1smote, credit_train_smote$DEFAULT)
print(conf_matrix_train1smote)

# conf matrix train
conf_matrix_test1smote <- confusionMatrix(predictions_test1smote, credit_test_smote$DEFAULT)
print(conf_matrix_test1smote)
```

# SMOTE AUC Train Set 2
```{r}
# train AUC for 800 trees, 20 depth
rf_AUC1smote <- ranger(formula = DEFAULT ~ ., data = credit_train_smote, num.trees = 800, max.depth = 20, probability = TRUE, importance = 'impurity', seed = 1234)

# probabilities
probs_train1smote <- predict(rf_AUC1smote, data = credit_train_smote)$predictions[, 2]

# predictions
pred_train1smote <- prediction(probs_train1smote, credit_train_smote$DEFAULT)

# tpr and fpr
perf_train1smote <- performance(pred_train1smote, measure = "tpr", x.measure = "fpr")

# auc performance
auc_perf_train1smote <- performance(pred_train1smote, measure = "auc")

# AUC calculation
auc_value_train1smote <- auc_perf_train1smote@y.values[[1]]

# print AUC value
print(paste("AUC =", round(auc_value_train1smote, 4)))

```

# SMOTE AUC Test Set 2
```{r}
# probabilities
probs_test1smote <- predict(rf_AUC1smote, data = credit_test_smote)$predictions[, 2]

# predictions
pred_test1smote <- prediction(probs_test1smote, credit_test_smote$DEFAULT)

# tpr and fpr
perf_test1smote <- performance(pred_test1smote, measure = "tpr", x.measure = "fpr")

# auc performance
auc_perf_test1smote <- performance(pred_test1smote, measure = "auc")

# AUC calculation
auc_value_test1smote <- auc_perf_test1smote@y.values[[1]]

# print AUC value
print(paste("AUC =", round(auc_value_test1smote, 4)))
```

Increasing the number of trees and depth to 800 trees, and a depth of 20, we observed an accuracy of 0.957, recall of 1.0, precision of 0.92, and an AUC 0.99. In the test set, we observed an accuracy of 0.955, recall of 1.0, precision of 0.92, and an AUC of 0.978. The AUC remained the same between the first and second model, but the accuracy and precision improved.

We can see that the SMOTE balancing method was much more effective for this type of data set, showing good generalizability between both models. Additionally, increasing the number of trees and depth resulted in an improved model while maintaining generalizability.

